{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split \n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, Flatten\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from keras.initializers import Constant \n",
    "from keras import Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN(data, wv_from_bin, batch_size=128, kernel_size=5):\n",
    "    \n",
    "    # Preparing Text Data and Label\n",
    "    tweet, label = data['text'], data['sentiment']\n",
    "    \n",
    "    # Tokenizing Text\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(tweet)\n",
    "    sequences = tokenizer.texts_to_sequences(tweet)\n",
    "\n",
    "    # Preparing Embedding Dictionary\n",
    "    word_index = tokenizer.word_index\n",
    "    tweets_pad = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "    embedding_index = {}\n",
    "    words = list(wv_from_bin.vocab.keys())\n",
    "    curInd = 0\n",
    "    for w in words:\n",
    "        try:\n",
    "            embedding_index[w] = wv_from_bin.word_vec(w)\n",
    "            curInd += 1\n",
    "        except KeyError:\n",
    "            continue\n",
    "            \n",
    "    # Preparing Embedding Matrix\n",
    "    embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, vec_size))\n",
    "    words = []\n",
    "    for word, i in word_index.items():\n",
    "        words.append(word)\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    # Preparing Embedding Layer\n",
    "    embedding_layer = Embedding(len(tokenizer.word_index) + 1,\n",
    "                            vec_size,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=max_length,\n",
    "                            trainable=False)\n",
    "    \n",
    "    # Construction CNN Model Structure\n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer)\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Conv1D(filters, kernel_size, padding='valid', activation='relu', strides=1))\n",
    "    model.add(Conv1D(300, kernel_size, padding='valid', activation='relu', strides=1))\n",
    "    model.add(Conv1D(150, kernel_size, padding='valid', activation='relu', strides=1))\n",
    "    model.add(Conv1D(75, kernel_size, padding='valid', activation='relu', strides=1))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(600))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('tanh'))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_error'])\n",
    "    filepath = \"/Users/colinwan/Desktop/DataFest2020/checkpoint/4cnn-{epoch:02d}-{loss:0.3f}-{mean_squared_error:0.3f}-{val_loss:0.3f}-{val_mean_squared_error:0.3f}.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor=\"loss\", verbose=1, save_best_only=True, mode='min')\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=0.000001)\n",
    "    \n",
    "    # Training Model\n",
    "    model.fit(tweets_pad, label, batch_size=batch_size, epochs=8, validation_split=0.1, shuffle=True, callbacks=[checkpoint, reduce_lr])\n",
    "    \n",
    "    # Saving Trained Model\n",
    "    model.save(\"/Users/colinwan/Desktop/DataFest2020/Models/model.h5\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12483 samples, validate on 1388 samples\n",
      "Epoch 1/8\n",
      "12483/12483 [==============================] - 107s 9ms/step - loss: 0.5431 - mean_squared_error: 0.5431 - val_loss: 0.4636 - val_mean_squared_error: 0.4636\n",
      "\n",
      "Epoch 00001: loss improved from inf to 0.54309, saving model to /Users/colinwan/Desktop/DataFest2020/checkpoint/4cnn-01-0.543-0.543-0.464-0.464.hdf5\n",
      "Epoch 2/8\n",
      "12483/12483 [==============================] - 103s 8ms/step - loss: 0.4528 - mean_squared_error: 0.4528 - val_loss: 0.4325 - val_mean_squared_error: 0.4325\n",
      "\n",
      "Epoch 00002: loss improved from 0.54309 to 0.45280, saving model to /Users/colinwan/Desktop/DataFest2020/checkpoint/4cnn-02-0.453-0.453-0.432-0.432.hdf5\n",
      "Epoch 3/8\n",
      "12483/12483 [==============================] - 111s 9ms/step - loss: 0.4113 - mean_squared_error: 0.4113 - val_loss: 0.4022 - val_mean_squared_error: 0.4022\n",
      "\n",
      "Epoch 00003: loss improved from 0.45280 to 0.41132, saving model to /Users/colinwan/Desktop/DataFest2020/checkpoint/4cnn-03-0.411-0.411-0.402-0.402.hdf5\n",
      "Epoch 4/8\n",
      "12483/12483 [==============================] - 126s 10ms/step - loss: 0.3693 - mean_squared_error: 0.3693 - val_loss: 0.4274 - val_mean_squared_error: 0.4274\n",
      "\n",
      "Epoch 00004: loss improved from 0.41132 to 0.36927, saving model to /Users/colinwan/Desktop/DataFest2020/checkpoint/4cnn-04-0.369-0.369-0.427-0.427.hdf5\n",
      "Epoch 5/8\n",
      "12483/12483 [==============================] - 118s 9ms/step - loss: 0.3276 - mean_squared_error: 0.3276 - val_loss: 0.4120 - val_mean_squared_error: 0.4120\n",
      "\n",
      "Epoch 00005: loss improved from 0.36927 to 0.32764, saving model to /Users/colinwan/Desktop/DataFest2020/checkpoint/4cnn-05-0.328-0.328-0.412-0.412.hdf5\n",
      "Epoch 6/8\n",
      "12483/12483 [==============================] - 118s 9ms/step - loss: 0.2516 - mean_squared_error: 0.2516 - val_loss: 0.4229 - val_mean_squared_error: 0.4229\n",
      "\n",
      "Epoch 00006: loss improved from 0.32764 to 0.25156, saving model to /Users/colinwan/Desktop/DataFest2020/checkpoint/4cnn-06-0.252-0.252-0.423-0.423.hdf5\n",
      "Epoch 7/8\n",
      "12483/12483 [==============================] - 133s 11ms/step - loss: 0.2142 - mean_squared_error: 0.2142 - val_loss: 0.4490 - val_mean_squared_error: 0.4490\n",
      "\n",
      "Epoch 00007: loss improved from 0.25156 to 0.21423, saving model to /Users/colinwan/Desktop/DataFest2020/checkpoint/4cnn-07-0.214-0.214-0.449-0.449.hdf5\n",
      "Epoch 8/8\n",
      "12483/12483 [==============================] - 107s 9ms/step - loss: 0.1870 - mean_squared_error: 0.1870 - val_loss: 0.4373 - val_mean_squared_error: 0.4373\n",
      "\n",
      "Epoch 00008: loss improved from 0.21423 to 0.18702, saving model to /Users/colinwan/Desktop/DataFest2020/checkpoint/4cnn-08-0.187-0.187-0.437-0.437.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x1a9fabe610>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CNN(tweets_pad, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
